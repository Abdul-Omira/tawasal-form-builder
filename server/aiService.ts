/**
 * ğŸ¤– Syrian Ministry AI Service
 * Local AI Integration with Llama 3.1/3.2 Models
 */

import { Ollama } from 'ollama';
import type { CitizenCommunication } from '@shared/schema';

// AI Configuration
const AI_CONFIG = {
  OLLAMA_HOST: process.env.OLLAMA_HOST || 'http://localhost:11434',
  PRIMARY_MODEL: process.env.AI_MODEL || 'llama3.2:latest',
  FALLBACK_MODEL: process.env.AI_FALLBACK_MODEL || 'llama3.1:8b',
  MAX_TOKENS: parseInt(process.env.AI_MAX_TOKENS || '2048'),
  TEMPERATURE: parseFloat(process.env.AI_TEMPERATURE || '0.7'),
  TIMEOUT: parseInt(process.env.AI_TIMEOUT || '30000'),
};

// Initialize Ollama client
let ollama: Ollama;
let isInitialized = false;

// AI Response Types
export interface AIResponse {
  success: boolean;
  response?: string;
  model?: string;
  tokens?: number;
  processingTime?: number;
  error?: string;
}

export interface AIAnalysis {
  summary: string;
  sentiment: 'positive' | 'negative' | 'neutral';
  urgency: 'low' | 'medium' | 'high' | 'critical';
  category: string;
  recommendations: string[];
  confidence: number;
}

// Initialize AI Service
export const initializeAIService = async (): Promise<boolean> => {
  try {
    console.log('ğŸ¤– [AI-INIT] Initializing Syrian Ministry AI Service...');
    console.log(`ğŸ¤– [AI-INIT] Ollama Host: ${AI_CONFIG.OLLAMA_HOST}`);

    ollama = new Ollama({ 
      host: AI_CONFIG.OLLAMA_HOST,
    });

    // Test connection
    const models = await ollama.list();
    console.log(`ğŸ¤– [AI-INIT] Available models: ${models.models.map(m => m.name).join(', ')}`);

    // Check if primary model exists
    const primaryModelExists = models.models.some(m => m.name === AI_CONFIG.PRIMARY_MODEL);
    if (!primaryModelExists) {
      console.warn(`âš ï¸ [AI-INIT] Primary model ${AI_CONFIG.PRIMARY_MODEL} not found`);
      const fallbackModelExists = models.models.some(m => m.name === AI_CONFIG.FALLBACK_MODEL);
      if (fallbackModelExists) {
        console.log(`ğŸ¤– [AI-INIT] Using fallback model: ${AI_CONFIG.FALLBACK_MODEL}`);
        AI_CONFIG.PRIMARY_MODEL = AI_CONFIG.FALLBACK_MODEL;
      } else {
        console.error('âŒ [AI-INIT] No suitable AI models found');
        return false;
      }
    }

    // Test model response
    const testResponse = await ollama.generate({
      model: AI_CONFIG.PRIMARY_MODEL,
      prompt: 'Ù…Ø±Ø­Ø¨Ø§ØŒ Ø£Ù†Ø§ Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ Ù„Ù„ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø³ÙˆØ±ÙŠØ©. Ù‚Ù„ Ù…Ø±Ø­Ø¨Ø§ ÙÙ‚Ø·.',
      options: { temperature: 0.1 }
    });

    if (testResponse.response) {
      console.log('âœ… [AI-INIT] AI model test successful');
      isInitialized = true;
      return true;
    } else {
      console.error('âŒ [AI-INIT] AI model test failed');
      return false;
    }

  } catch (error) {
    console.error('âŒ [AI-INIT] Failed to initialize AI service:', error);
    return false;
  }
};

// Analyze citizen communication
export const analyzeCommunication = async (communication: CitizenCommunication): Promise<AIAnalysis | null> => {
  if (!isInitialized) {
    console.warn('âš ï¸ [AI-ANALYZE] AI service not initialized');
    return null;
  }

  try {
    console.log(`ğŸ¤– [AI-ANALYZE] Analyzing communication #${communication.id}`);
    
    const analysisPrompt = `
Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø°ÙƒÙŠ Ù„Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ© ÙÙŠ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ø³ÙˆØ±ÙŠØ©. Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:

Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: ${communication.subject}
Ø§Ù„Ø±Ø³Ø§Ù„Ø©: ${communication.message}
Ù†ÙˆØ¹ Ø§Ù„ØªÙˆØ§ØµÙ„: ${communication.communicationType}

Ù‚Ù… Ø¨ØªÙ‚Ø¯ÙŠÙ… ØªØ­Ù„ÙŠÙ„ ÙŠØªØ¶Ù…Ù†:
1. Ù…Ù„Ø®Øµ Ù…Ø®ØªØµØ±
2. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (positive/negative/neutral)
3. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© (low/medium/high/critical)
4. Ø§Ù„ØªØµÙ†ÙŠÙ
5. ØªÙˆØµÙŠØ§Øª Ù„Ù„Ø±Ø¯
6. Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© (0-100%)

Ù‚Ø¯Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨ØµÙŠØºØ© JSON:
{
  "summary": "Ø§Ù„Ù…Ù„Ø®Øµ Ù‡Ù†Ø§",
  "sentiment": "positive/negative/neutral",
  "urgency": "low/medium/high/critical",
  "category": "Ø§Ù„ØªØµÙ†ÙŠÙ Ù‡Ù†Ø§",
  "recommendations": ["ØªÙˆØµÙŠØ© 1", "ØªÙˆØµÙŠØ© 2", "ØªÙˆØµÙŠØ© 3"],
  "confidence": 85
}
`;

    const response = await ollama.generate({
      model: AI_CONFIG.PRIMARY_MODEL,
      prompt: analysisPrompt,
      options: {
        temperature: AI_CONFIG.TEMPERATURE,
        num_predict: AI_CONFIG.MAX_TOKENS,
      }
    });

    try {
      const cleanResponse = response.response
        .replace(/```json\n?/g, '')
        .replace(/```\n?/g, '')
        .trim();
      
      const analysis = JSON.parse(cleanResponse);
      
      return {
        summary: analysis.summary || 'ØªØ­Ù„ÙŠÙ„ ØºÙŠØ± Ù…ØªÙˆÙØ±',
        sentiment: ['positive', 'negative', 'neutral'].includes(analysis.sentiment) 
          ? analysis.sentiment : 'neutral',
        urgency: ['low', 'medium', 'high', 'critical'].includes(analysis.urgency) 
          ? analysis.urgency : 'medium',
        category: analysis.category || 'Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ø§Ù…',
        recommendations: Array.isArray(analysis.recommendations) 
          ? analysis.recommendations.slice(0, 5) : ['Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø·Ù„Ø¨', 'Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ù…ÙˆØ§Ø·Ù†', 'ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ø±Ø¯'],
        confidence: Math.min(100, Math.max(0, analysis.confidence || 70))
      };

    } catch (parseError) {
      console.warn('âš ï¸ [AI-ANALYZE] Failed to parse JSON, using fallback');
      const response_text = response.response.toLowerCase();
      
      return {
        summary: response.response.substring(0, 200) + '...',
        sentiment: response_text.includes('Ø³Ù„Ø¨ÙŠ') || response_text.includes('Ø´ÙƒÙˆÙ‰') ? 'negative' :
                  response_text.includes('Ø¥ÙŠØ¬Ø§Ø¨ÙŠ') || response_text.includes('Ø´ÙƒØ±') ? 'positive' : 'neutral',
        urgency: response_text.includes('Ø¹Ø§Ø¬Ù„') || response_text.includes('Ø¹Ø§Ù„ÙŠ') ? 'high' :
                response_text.includes('Ù…Ù†Ø®ÙØ¶') ? 'low' : 'medium',
        category: communication.communicationType || 'Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ø§Ù…',
        recommendations: ['Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù„Ø·Ù„Ø¨', 'Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ù…ÙˆØ§Ø·Ù†', 'ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨'],
        confidence: 60
      };
    }

  } catch (error) {
    console.error('âŒ [AI-ANALYZE] Analysis failed:', error);
    return null;
  }
};

// Generate response suggestions
export const generateResponseSuggestions = async (
  communication: CitizenCommunication,
  context?: string
): Promise<string[]> => {
  if (!isInitialized) {
    return ['Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹. Ø³ÙŠØªÙ… Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ø³ØªÙØ³Ø§Ø±ÙƒÙ… Ù‚Ø±ÙŠØ¨Ø§Ù‹.'];
  }

  try {
    const suggestionPrompt = `
Ø£Ù†Øª Ù…ÙˆØ¸Ù Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ Ù…Ø­ØªØ±Ù ÙÙŠ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø§Ù„Ø³ÙˆØ±ÙŠØ©.

Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…ÙˆØ§Ø·Ù†:
Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹: ${communication.subject}
Ø§Ù„Ù…Ø­ØªÙˆÙ‰: ${communication.message}

Ø§ÙƒØªØ¨ 3 Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„Ø±Ø¯ Ø¨Ù„Ù‡Ø¬Ø© Ø±Ø³Ù…ÙŠØ© ÙˆÙ…Ù‡Ø°Ø¨Ø©:

1. Ø±Ø¯ Ù…Ø®ØªØµØ± ÙˆÙ…Ø¨Ø§Ø´Ø±
2. Ø±Ø¯ ØªÙØµÙŠÙ„ÙŠ Ù…Ø¹ Ø´Ø±Ø­  
3. Ø±Ø¯ ÙŠØ·Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©

ÙƒÙ„ Ø±Ø¯ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¨Ø¯Ø£ Ø¨ØªØ­ÙŠØ© ÙˆÙŠÙ†ØªÙ‡ÙŠ Ø¨ØªÙˆÙ‚ÙŠØ¹ Ø±Ø³Ù…ÙŠ.
Ù‚Ø¯Ù… ÙƒÙ„ Ø§Ù‚ØªØ±Ø§Ø­ ÙÙŠ Ø³Ø·Ø± Ù…Ù†ÙØµÙ„ Ù…Ø¹ Ø±Ù‚Ù…Ù‡.
`;

    const response = await ollama.generate({
      model: AI_CONFIG.PRIMARY_MODEL,
      prompt: suggestionPrompt,
      options: {
        temperature: 0.8,
        num_predict: AI_CONFIG.MAX_TOKENS,
      }
    });

    const suggestions = response.response
      .split(/\d+\.\s+/)
      .filter(suggestion => suggestion.trim().length > 50)
      .map(suggestion => suggestion.trim())
      .slice(0, 3);

    if (suggestions.length === 0) {
      return [
        'ØªØ­ÙŠØ© Ø·ÙŠØ¨Ø©ØŒ Ø´ÙƒØ±Ø§Ù‹ Ù„ØªÙˆØ§ØµÙ„ÙƒÙ… Ù…Ø¹ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª. Ø³ÙŠØªÙ… Ø¯Ø±Ø§Ø³Ø© Ø·Ù„Ø¨ÙƒÙ… ÙˆØ§Ù„Ø±Ø¯ Ø¹Ù„ÙŠÙƒÙ… Ù‚Ø±ÙŠØ¨Ø§Ù‹. Ù…Ø¹ ÙØ§Ø¦Ù‚ Ø§Ù„Ø§Ø­ØªØ±Ø§Ù…ØŒ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª ÙˆØªÙ‚Ø§Ù†Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.',
        'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…ØŒ Ù†Ø´ÙƒØ±ÙƒÙ… Ø¹Ù„Ù‰ Ø«Ù‚ØªÙƒÙ… Ø¨ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª. ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø±Ø³Ø§Ù„ØªÙƒÙ… ÙˆØ³ÙŠØªÙ… Ù…Ø±Ø§Ø¬Ø¹ØªÙ‡Ø§ Ù…Ù† Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ù…Ø®ØªØµ. ÙˆØªÙ‚Ø¨Ù„ÙˆØ§ ÙØ§Ø¦Ù‚ Ø§Ù„Ø§Ø­ØªØ±Ø§Ù….',
        'ØªØ­ÙŠØ© Ø·ÙŠØ¨Ø©ØŒ ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø·Ù„Ø¨ÙƒÙ… ÙˆØ³ÙŠØªÙ… Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙƒÙ… Ù„Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±. Ø´ÙƒØ±Ø§Ù‹ Ù„ØªØ¹Ø§ÙˆÙ†ÙƒÙ…ØŒ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª ÙˆØªÙ‚Ø§Ù†Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.'
      ];
    }

    return suggestions;

  } catch (error) {
    console.error('âŒ [AI-SUGGEST] Failed to generate suggestions:', error);
    return ['Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ. Ø³ÙŠØªÙ… Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ø³ØªÙØ³Ø§Ø±ÙƒÙ… Ø¨ÙˆØ§Ø³Ø·Ø© ÙØ±ÙŠÙ‚ Ø§Ù„Ø¯Ø¹Ù… Ù‚Ø±ÙŠØ¨Ø§Ù‹.'];
  }
};

// Get AI service status
export const getAIStatus = async () => {
  try {
    const models = ollama ? await ollama.list() : { models: [] };
    
    return {
      initialized: isInitialized,
      model: AI_CONFIG.PRIMARY_MODEL,
      host: AI_CONFIG.OLLAMA_HOST,
      modelsAvailable: models.models.map(m => m.name),
      lastCheck: new Date().toISOString()
    };
  } catch (error) {
    return {
      initialized: false,
      model: AI_CONFIG.PRIMARY_MODEL,
      host: AI_CONFIG.OLLAMA_HOST,
      modelsAvailable: [],
      lastCheck: new Date().toISOString()
    };
  }
};

// Health check
export const healthCheck = async (): Promise<boolean> => {
  try {
    if (!isInitialized || !ollama) {
      return false;
    }

    const testResponse = await ollama.generate({
      model: AI_CONFIG.PRIMARY_MODEL,
      prompt: 'Ø§Ø®ØªØ¨Ø§Ø±',
      options: { temperature: 0.1, num_predict: 10 }
    });

    return testResponse.response !== undefined;
  } catch (error) {
    console.error('âŒ [AI-HEALTH] Health check failed:', error);
    return false;
  }
};

// AI Chat interface
export const aiChat = async (message: string, context?: string): Promise<AIResponse> => {
  if (!isInitialized) {
    return {
      success: false,
      error: 'AI service not initialized'
    };
  }

  try {
    const startTime = Date.now();
    
    const chatPrompt = `
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª ÙˆØªÙ‚Ø§Ù†Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙˆØ±ÙŠØ©.
${context ? `Ø§Ù„Ø³ÙŠØ§Ù‚: ${context}` : ''}

Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: ${message}

Ø£Ø¬Ø¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ÙÙŠØ¯Ø© ÙˆÙ…Ù‡Ù†ÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:
`;

    const response = await ollama.generate({
      model: AI_CONFIG.PRIMARY_MODEL,
      prompt: chatPrompt,
      options: {
        temperature: AI_CONFIG.TEMPERATURE,
        num_predict: AI_CONFIG.MAX_TOKENS,
      }
    });

    const processingTime = Date.now() - startTime;

    return {
      success: true,
      response: response.response,
      model: AI_CONFIG.PRIMARY_MODEL,
      tokens: response.response.length,
      processingTime
    };

  } catch (error) {
    console.error('âŒ [AI-CHAT] Chat failed:', error);
    return {
      success: false,
      error: 'Failed to process AI chat request'
    };
  }
}; 